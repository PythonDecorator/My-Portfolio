{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsRCxAeAnVVnEwDcVd65N/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PythonDecorator/My-Portfolio/blob/master/Loan_Default_and_Credit_Risk_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "Mr3D8MrP33fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "x7_Xye5w4CLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d602072-e30e-4c29-dc1f-91756bf9e2d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statement"
      ],
      "metadata": {
        "id": "tg81eTbiXAkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l93Q1TltW6F9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# for displaying markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    brier_score_loss,\n",
        "    cohen_kappa_score,\n",
        "    confusion_matrix,\n",
        "    RocCurveDisplay,\n",
        "    matthews_corrcoef,\n",
        "    RocCurveDisplay,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, learning_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# others insatll\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# for predicting best features\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount GDrive"
      ],
      "metadata": {
        "id": "JJ4beadnefj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Tfm1FvjueedO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "WiIKsD9Hdo09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_accepted = '/content/drive/MyDrive/MY WORK/ML/LoanDefault/accepted_2007_to_2018Q4.csv'\n",
        "file_path_rejected = '/content/drive/MyDrive/MY WORK/ML/LoanDefault/rejected_2007_to_2018Q4.csv'"
      ],
      "metadata": {
        "id": "zdUebMjMdq37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "\n",
        "    def load_csv(self, usecols: list = [], dtypes: dict = {}, chunksize: int = 0, nrows: int = 0):\n",
        "        \"\"\"\n",
        "        Load CSV with optional column selection, dtypes, chunking, and limited rows.\n",
        "        \"\"\"\n",
        "        if nrows:\n",
        "            self.df = pd.read_csv(self.filepath, usecols=usecols or None, dtype=dtypes or None, nrows=nrows)\n",
        "        elif chunksize:\n",
        "            chunks = []\n",
        "            for chunk in pd.read_csv(self.filepath, usecols=usecols or None, dtype=dtypes or None, chunksize=chunksize):\n",
        "                chunks.append(chunk)\n",
        "            self.df = pd.concat(chunks, ignore_index=True)\n",
        "        else:\n",
        "            self.df = pd.read_csv(self.filepath, usecols=usecols or None, dtype=dtypes or None)\n",
        "        return self.df\n",
        "\n",
        "    def preview_columns(self, start_col=0, ncols=10, nrows=10):\n",
        "        \"\"\"\n",
        "        Preview a subset of the dataframe:\n",
        "        - First nrows\n",
        "        - Columns from start_col to start_col + ncols\n",
        "        \"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Dataframe not loaded. Call load_csv() first.\")\n",
        "        end_col = start_col + ncols\n",
        "        return self.df.iloc[:nrows, start_col:end_col]\n"
      ],
      "metadata": {
        "id": "BPetgL5bf4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\n",
        "    # 'id',\n",
        "    'loan_amnt',\n",
        "    'funded_amnt',\n",
        "    'funded_amnt_inv',\n",
        "    'term',\n",
        "    'int_rate',\n",
        "    'installment',\n",
        "    'grade',\n",
        "    'sub_grade',\n",
        "    # 'emp_title',\n",
        "    'emp_length',\n",
        "    'home_ownership',\n",
        "    'annual_inc',\n",
        "    'verification_status',\n",
        "    # 'issue_d',\n",
        "    'loan_status',\n",
        "    'pymnt_plan',\n",
        "    # 'url',\n",
        "    'purpose',\n",
        "    # 'title',\n",
        "    # 'zip_code',\n",
        "    # 'addr_state',\n",
        "    'dti',\n",
        "    'delinq_2yrs',\n",
        "    # 'earliest_cr_line',\n",
        "    'fico_range_low',\n",
        "    'fico_range_high',\n",
        "    'inq_last_6mths',\n",
        "    'open_acc',\n",
        "    'pub_rec',\n",
        "    'revol_bal',\n",
        "    'revol_util',\n",
        "    'total_acc',\n",
        "    'initial_list_status',\n",
        "    'out_prncp',\n",
        "    'out_prncp_inv',\n",
        "    'total_pymnt',\n",
        "    'total_pymnt_inv',\n",
        "    'total_rec_prncp',\n",
        "    'total_rec_int',\n",
        "    'total_rec_late_fee',\n",
        "    'recoveries',\n",
        "    'collection_recovery_fee',\n",
        "    # 'last_pymnt_d',\n",
        "    'last_pymnt_amnt',\n",
        "    # 'last_credit_pull_d',\n",
        "    'last_fico_range_high',\n",
        "    'last_fico_range_low',\n",
        "    'collections_12_mths_ex_med',\n",
        "    'mths_since_last_delinq',\n",
        "    'policy_code',\n",
        "    'application_type',\n",
        "    'acc_now_delinq',\n",
        "    'tot_coll_amt',\n",
        "    'tot_cur_bal',\n",
        "    'open_acc_6m',\n",
        "    'open_act_il',\n",
        "    'open_il_12m',\n",
        "    'open_il_24m',\n",
        "    'mths_since_rcnt_il',\n",
        "    'total_bal_il',\n",
        "    # 'il_util',\n",
        "    'open_rv_12m',\n",
        "    'open_rv_24m',\n",
        "    'max_bal_bc',\n",
        "    'all_util',\n",
        "    'total_rev_hi_lim',\n",
        "    'inq_fi',\n",
        "    'total_cu_tl',\n",
        "    'inq_last_12m',\n",
        "    'acc_open_past_24mths',\n",
        "    'avg_cur_bal',\n",
        "    'bc_open_to_buy',\n",
        "    'bc_util',\n",
        "    'chargeoff_within_12_mths',\n",
        "    'delinq_amnt',\n",
        "    'mo_sin_old_il_acct',\n",
        "    'mo_sin_old_rev_tl_op',\n",
        "    'mo_sin_rcnt_rev_tl_op',\n",
        "    'mo_sin_rcnt_tl',\n",
        "    'mort_acc',\n",
        "    'mths_since_recent_bc',\n",
        "    'mths_since_recent_inq',\n",
        "    'num_accts_ever_120_pd',\n",
        "    'num_actv_bc_tl',\n",
        "    'num_actv_rev_tl',\n",
        "    'num_bc_sats',\n",
        "    'num_bc_tl',\n",
        "    'num_il_tl',\n",
        "    'num_op_rev_tl',\n",
        "    'num_rev_accts',\n",
        "    'num_rev_tl_bal_gt_0',\n",
        "    'num_sats',\n",
        "    # 'num_tl_120dpd_2m',\n",
        "    'num_tl_30dpd',\n",
        "    'num_tl_90g_dpd_24m',\n",
        "    'num_tl_op_past_12m',\n",
        "    'pct_tl_nvr_dlq',\n",
        "    'percent_bc_gt_75',\n",
        "    'pub_rec_bankruptcies',\n",
        "    'tax_liens',\n",
        "    'tot_hi_cred_lim',\n",
        "    'total_bal_ex_mort',\n",
        "    'total_bc_limit',\n",
        "    'total_il_high_credit_limit',\n",
        "    'hardship_flag',\n",
        "    'disbursement_method',\n",
        "    'debt_settlement_flag'\n",
        "]"
      ],
      "metadata": {
        "id": "CyPAT2G3jgEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes = {\n",
        "    'loan_amnt': 'float64',\n",
        "    'funded_amnt': 'float64',\n",
        "    'funded_amnt_inv': 'float64',\n",
        "    'term': 'object',\n",
        "    'int_rate': 'float64',\n",
        "    'installment': 'float64',\n",
        "    'grade': 'object',\n",
        "    'sub_grade': 'object',\n",
        "    'emp_title': 'object',\n",
        "    'emp_length': 'object',\n",
        "    'home_ownership': 'object',\n",
        "    'annual_inc': 'float64',\n",
        "    'verification_status': 'object',\n",
        "    'issue_d': 'object',\n",
        "    'loan_status': 'object',\n",
        "    'pymnt_plan': 'object',\n",
        "    'purpose': 'object',\n",
        "    'dti': 'float64',\n",
        "    'delinq_2yrs': 'float64',\n",
        "    'earliest_cr_line': 'object',\n",
        "    'fico_range_low': 'float64',\n",
        "    'fico_range_high': 'float64',\n",
        "    'inq_last_6mths': 'float64',\n",
        "    'open_acc': 'float64',\n",
        "    'pub_rec': 'float64',\n",
        "    'revol_bal': 'float64',\n",
        "    'revol_util': 'float64',\n",
        "    'total_acc': 'float64',\n",
        "    'initial_list_status': 'object',\n",
        "    'out_prncp': 'float64',\n",
        "    'out_prncp_inv': 'float64',\n",
        "    'total_pymnt': 'float64',\n",
        "    'total_pymnt_inv': 'float64',\n",
        "    'total_rec_prncp': 'float64',\n",
        "    'total_rec_int': 'float64',\n",
        "    'total_rec_late_fee': 'float64',\n",
        "    'recoveries': 'float64',\n",
        "    'collection_recovery_fee': 'float64',\n",
        "    'last_pymnt_d': 'object',\n",
        "    'last_pymnt_amnt': 'float64',\n",
        "    'last_credit_pull_d': 'object',\n",
        "    'last_fico_range_high': 'float64',\n",
        "    'last_fico_range_low': 'float64',\n",
        "    'collections_12_mths_ex_med': 'float64',\n",
        "    'mths_since_last_delinq': 'float64',\n",
        "    'policy_code': 'float64',\n",
        "    'application_type': 'object',\n",
        "    'acc_now_delinq': 'float64',\n",
        "    'tot_coll_amt': 'float64',\n",
        "    'tot_cur_bal': 'float64',\n",
        "    'open_acc_6m': 'float64',\n",
        "    'open_act_il': 'float64',\n",
        "    'open_il_12m': 'float64',\n",
        "    'open_il_24m': 'float64',\n",
        "    'mths_since_rcnt_il': 'float64',\n",
        "    'total_bal_il': 'float64',\n",
        "    'open_rv_12m': 'float64',\n",
        "    'open_rv_24m': 'float64',\n",
        "    'max_bal_bc': 'float64',\n",
        "    'all_util': 'float64',\n",
        "    'total_rev_hi_lim': 'float64',\n",
        "    'inq_fi': 'float64',\n",
        "    'total_cu_tl': 'float64',\n",
        "    'inq_last_12m': 'float64',\n",
        "    'acc_open_past_24mths': 'float64',\n",
        "    'avg_cur_bal': 'float64',\n",
        "    'bc_open_to_buy': 'float64',\n",
        "    'bc_util': 'float64',\n",
        "    'chargeoff_within_12_mths': 'float64',\n",
        "    'delinq_amnt': 'float64',\n",
        "    'mo_sin_old_il_acct': 'float64',\n",
        "    'mo_sin_old_rev_tl_op': 'float64',\n",
        "    'mo_sin_rcnt_rev_tl_op': 'float64',\n",
        "    'mo_sin_rcnt_tl': 'float64',\n",
        "    'mort_acc': 'float64',\n",
        "    'mths_since_recent_bc': 'float64',\n",
        "    'mths_since_recent_inq': 'float64',\n",
        "    'num_accts_ever_120_pd': 'float64',\n",
        "    'num_actv_bc_tl': 'float64',\n",
        "    'num_actv_rev_tl': 'float64',\n",
        "    'num_bc_sats': 'float64',\n",
        "    'num_bc_tl': 'float64',\n",
        "    'num_il_tl': 'float64',\n",
        "    'num_op_rev_tl': 'float64',\n",
        "    'num_rev_accts': 'float64',\n",
        "    'num_rev_tl_bal_gt_0': 'float64',\n",
        "    'num_sats': 'float64',\n",
        "    'num_tl_30dpd': 'float64',\n",
        "    'num_tl_90g_dpd_24m': 'float64',\n",
        "    'num_tl_op_past_12m': 'float64',\n",
        "    'pct_tl_nvr_dlq': 'float64',\n",
        "    'percent_bc_gt_75': 'float64',\n",
        "    'pub_rec_bankruptcies': 'float64',\n",
        "    'tax_liens': 'float64',\n",
        "    'tot_hi_cred_lim': 'float64',\n",
        "    'total_bal_ex_mort': 'float64',\n",
        "    'total_bc_limit': 'float64',\n",
        "    'total_il_high_credit_limit': 'float64',\n",
        "    'hardship_flag': 'object',\n",
        "    'disbursement_method': 'object',\n",
        "    'debt_settlement_flag': 'object'\n",
        "}"
      ],
      "metadata": {
        "id": "xAwfmtJAutbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(file_path_accepted)\n",
        "accepted_df = data_loader.load_csv(usecols=cols, dtypes=dtypes, chunksize=100_000)"
      ],
      "metadata": {
        "id": "C4uEYlzSktha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization EDA"
      ],
      "metadata": {
        "id": "Cc9jStOtXqMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataVisualization:\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        self.data = data\n",
        "\n",
        "    def basic_info(self):\n",
        "        \"\"\"Show shape, columns, head/tail, duplicates.\"\"\"\n",
        "        display(Markdown(\"## Basic Info\"))\n",
        "        display(Markdown(f\"- Shape: **{self.data.shape}**\"))\n",
        "\n",
        "        display(Markdown(\"### Top 5 rows\"))\n",
        "        display(self.data.head())\n",
        "\n",
        "        display(Markdown(\"### Duplicate rows\"))\n",
        "        duplicates = self.data[self.data.duplicated(keep=False)]\n",
        "        display(duplicates)\n",
        "\n",
        "    def export_combined_data_summary(self, filename: str = \"data_summary.csv\", data = None):\n",
        "        \"\"\"Summary of each column saved to CSV and displayed as Markdown table.\"\"\"\n",
        "        if data is None:\n",
        "            data = self.data\n",
        "\n",
        "        summary = pd.DataFrame({\n",
        "            \"Column Name\": self.data.columns,\n",
        "            \"First Row\": self.data.iloc[0].values,\n",
        "            \"Last Row\": self.data.iloc[-1].values,\n",
        "            \"Data Type\": self.data.dtypes.values,\n",
        "            \"Missing Count\": self.data.isnull().sum().values,\n",
        "            \"Missing %\": (self.data.isnull().sum() / len(self.data) * 100).round(2).values,\n",
        "            \"Unique Values\": self.data.nunique().values\n",
        "        })\n",
        "        summary.to_csv(filename, index=False)\n",
        "        # display(Markdown(f\"✅ Combined data summary saved to **{filename}**\"))\n",
        "\n",
        "        # md_table = \"## Combined Data Summary\\n\"\n",
        "        # md_table += \"| Column Name | First Row | Last Row | Data Type | Missing Count | Missing % | Unique Values |\\n\"\n",
        "        # md_table += \"|---|---|---|---|---|---|---|\\n\"\n",
        "        # for _, row in summary.iterrows():\n",
        "        #     md_table += f\"| {row['Column Name']} | {row['First Row']} | {row['Last Row']} | {row['Data Type']} | {row['Missing Count']} | {row['Missing %']} | {row['Unique Values']} |\\n\"\n",
        "        # display(Markdown(md_table))\n",
        "\n",
        "    def describe_numeric(self, columns=None):\n",
        "        \"\"\"Describe numeric columns (rounded to 2 decimals).\"\"\"\n",
        "        cols = columns if columns else self.data.select_dtypes(include=np.number).columns\n",
        "        display(Markdown(\"## Numeric Summary\"))\n",
        "        display(self.data[cols].describe().round(2))\n",
        "\n",
        "    def outliers_zscore(self, column: str, threshold=3):\n",
        "        \"\"\"Show rows that are outliers in `column` by Z-score.\"\"\"\n",
        "        display(Markdown(f\"## Outliers in `{column}` (|Z|>{threshold})\"))\n",
        "        z_scores = np.abs(stats.zscore(self.data[column].dropna()))\n",
        "        outliers = self.data.loc[z_scores.index[z_scores > threshold]]\n",
        "        display(outliers)\n",
        "\n",
        "    def unique_values(self, columns: list):\n",
        "        \"\"\"Display unique values for a list of columns as a Markdown table.\"\"\"\n",
        "        display(Markdown(\"## Unique Values\"))\n",
        "        max_len = max(self.data[col].nunique() for col in columns)\n",
        "        md_table = \"| \" + \" | \".join(columns) + \" |\\n\"\n",
        "        md_table += \"| \" + \" | \".join([\"---\"] * len(columns)) + \" |\\n\"\n",
        "        for i in range(max_len):\n",
        "            row = []\n",
        "            for col in columns:\n",
        "                uniques = self.data[col].unique()\n",
        "                row.append(str(uniques[i]) if i < len(uniques) else \"-\")\n",
        "            md_table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
        "        display(Markdown(md_table))\n",
        "\n",
        "    def plot_bar_kde(self, columns: list):\n",
        "        \"\"\"Plot histogram + KDE for numeric columns.\"\"\"\n",
        "        for col in columns:\n",
        "            plt.figure(figsize=(6,4))\n",
        "            sns.histplot(self.data[col], kde=True)\n",
        "            plt.title(f\"Distribution of {col}\")\n",
        "            plt.show()\n",
        "\n",
        "    def plot_scatter(self, x: str, y: str, hue = None):\n",
        "        \"\"\"Plot scatter between two numeric columns, optionally with hue.\"\"\"\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.scatterplot(data=self.data, x=x, y=y, hue=hue, alpha=0.6)\n",
        "        plt.title(f\"{x} vs {y}\")\n",
        "        plt.show()\n",
        "\n",
        "    def run(self, numeric_cols=None, outlier_col=None, unique_cols=None, bar_cols=None, scatter=None):\n",
        "        \"\"\"\n",
        "        Run a quick full EDA pipeline.\n",
        "        \"\"\"\n",
        "        self.basic_info()\n",
        "        self.export_combined_data_summary()\n",
        "\n",
        "        if numeric_cols:\n",
        "            self.describe_numeric(numeric_cols)\n",
        "        if outlier_col:\n",
        "            self.outliers_zscore(outlier_col)\n",
        "        if unique_cols:\n",
        "            self.unique_values(unique_cols)\n",
        "        if bar_cols:\n",
        "            self.plot_bar_kde(bar_cols)\n",
        "        if scatter:\n",
        "            self.plot_scatter(*scatter)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gDJW4flqXwC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda = DataVisualization(accepted_df)\n",
        "eda.run()\n"
      ],
      "metadata": {
        "id": "ZCnGu9r5dge-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "hJvIg30qMheP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataCleaner:\n",
        "    \"\"\"Class for cleaning and preprocessing a DataFrame.\"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        # copy the data to avoid changing the original data\n",
        "        self.data = data.copy()\n",
        "\n",
        "    def remove_duplicates(self):\n",
        "        \"\"\"Remove duplicate rows from the DataFrame.\"\"\"\n",
        "        self.data.drop_duplicates(inplace=True)\n",
        "\n",
        "    def drop_cols_with_over_30_missing(self, threshold: float = 0.3):\n",
        "        \"\"\"\n",
        "        Drop columns with more than `threshold` missing values.\n",
        "        Default threshold = 0.3 (30%).\n",
        "        \"\"\"\n",
        "        missing_fraction = self.data.isna().mean()\n",
        "        cols_to_drop = missing_fraction[missing_fraction > threshold].index\n",
        "        self.data.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    def normalize_emp_length_to_numeric(self):\n",
        "        \"\"\"\n",
        "        Convert 'emp_length' column to numeric values.\n",
        "        '10+ years' -> 10\n",
        "        '3 years'   -> 3\n",
        "        -> np.nan (then filled)\n",
        "        \"\"\"\n",
        "        if 'emp_length' in self.data.columns:\n",
        "            def convert_emp_length(val):\n",
        "                if pd.isna(val):\n",
        "                    return np.nan\n",
        "                val = str(val).lower().strip()\n",
        "                if '<' in val:          # '< 1 year'\n",
        "                    return 0.5\n",
        "                if '10+' in val:        # '10+ years'\n",
        "                    return 10\n",
        "                if 'n/a' in val or 'na' == val:\n",
        "                    return np.nan\n",
        "                digits = ''.join(c for c in val if c.isdigit())\n",
        "                return float(digits) if digits else np.nan\n",
        "\n",
        "            self.data['emp_length'] = self.data['emp_length'].apply(convert_emp_length)\n",
        "\n",
        "            # fill missing with mean (or other estimate)\n",
        "            mean_val = self.data['emp_length'].mean()\n",
        "            self.data['emp_length'].fillna(mean_val, inplace=True)\n",
        "\n",
        "    def fill_other_missing_values(self):\n",
        "        \"\"\"\n",
        "        Fill missing values:\n",
        "        - For numeric columns, fill with mean.\n",
        "        - For object/categorical columns, fill with mode.\n",
        "        \"\"\"\n",
        "        for col in self.data.columns:\n",
        "            if self.data[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
        "                self.data[col].fillna(self.data[col].mean(), inplace=True)\n",
        "            else:\n",
        "                mode_val = self.data[col].mode()\n",
        "                if not mode_val.empty:\n",
        "                    self.data[col].fillna(mode_val[0], inplace=True)\n",
        "\n",
        "    def convert_to_categorical(self, columns=None):\n",
        "        \"\"\"\n",
        "        Convert specified columns to categorical dtype.\n",
        "        If columns is None, convert all object/string columns.\n",
        "        \"\"\"\n",
        "        if columns is None:\n",
        "            # Convert all object dtype columns\n",
        "            cols_to_convert = self.data.select_dtypes(include='object').columns\n",
        "        else:\n",
        "            cols_to_convert = columns\n",
        "\n",
        "        for col in cols_to_convert:\n",
        "            self.data[col] = self.data[col].astype('category')\n",
        "\n",
        "    def export_combined_data_summary(self, filename: str = \"final_data_summary.csv\"):\n",
        "        \"\"\"Summary of each column saved to CSV and displayed as Markdown table.\"\"\"\n",
        "        summary = pd.DataFrame({\n",
        "            \"Column Name\": self.data.columns,\n",
        "            \"First Row\": self.data.iloc[0].values,\n",
        "            \"Last Row\": self.data.iloc[-1].values,\n",
        "            \"Data Type\": self.data.dtypes.values,\n",
        "            \"Missing Count\": self.data.isnull().sum().values,\n",
        "            \"Missing %\": (self.data.isnull().sum() / len(self.data) * 100).round(2).values,\n",
        "            \"Unique Values\": self.data.nunique().values\n",
        "        })\n",
        "        summary.to_csv(filename, index=False)\n",
        "\n",
        "    def clean(self, filename: str = \"final_data_summary.csv\"):\n",
        "        \"\"\"Run the full cleaning pipeline.\"\"\"\n",
        "        self.remove_duplicates()\n",
        "        self.drop_cols_with_over_30_missing()  # default 30% threshold\n",
        "        self.normalize_emp_length_to_numeric()\n",
        "        self.fill_other_missing_values()\n",
        "        self.convert_to_categorical()\n",
        "        self.export_combined_data_summary(filename=filename)\n",
        "\n",
        "    def get_cleaned_data(self):\n",
        "        \"\"\"Get the cleaned DataFrame.\"\"\"\n",
        "        print(f\"✅ Data cleaning complete! - Shape {self.data.shape}\")\n",
        "        return self.data\n"
      ],
      "metadata": {
        "id": "ky8t3uwfMhA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Accepted Cleaded Data"
      ],
      "metadata": {
        "id": "IuPnC5SfjsoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaner instance\n",
        "data_cleaner = DataCleaner(accepted_df)\n",
        "\n",
        "# perform data cleaning\n",
        "data_cleaner.clean()\n",
        "\n",
        "# get the cleaned data\n",
        "cleaned_accepted_df = data_cleaner.get_cleaned_data()"
      ],
      "metadata": {
        "id": "fpdHrXmGTR1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the target col, loan_status\n",
        "cleaned_accepted_df['loan_status'].value_counts()"
      ],
      "metadata": {
        "id": "gXwXwsFYs5qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loan Status Distribution\n",
        "\n",
        "- **Good Loans (Fully Paid + DNM FP):** 1,078,740 (~78%)\n",
        "- **Bad Loans (Charged Off + Default + Late + In Grace + DNM CO):** 303,612 (~22%)\n",
        "- **Current Loans:** 878,317 (kept as separate test set)\n",
        "\n",
        "## Observations\n",
        "\n",
        "- Bad loans are a minority → class imbalance is expected.\n",
        "- Model will see more “good” examples than “bad” during training.\n",
        "- Current loans are kept separate to evaluate future defaults.\n",
        "\n",
        "## Handling Class Imbalance\n",
        "\n",
        "- Use **class weights** in the model (`class_weight='balanced'` in sklearn).  \n",
        "- **Oversample** bad loans (SMOTE, RandomOverSampler).  \n",
        "- **Undersample** good loans (with caution).  \n",
        "- Evaluate with metrics robust to imbalance: **ROC-AUC**, **Precision-Recall**.\n"
      ],
      "metadata": {
        "id": "x8qbc82jxnih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split cleaned data to test and train"
      ],
      "metadata": {
        "id": "sPX7qYPtyTnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataSplitter:\n",
        "    \"\"\"\n",
        "    Prepare Lending Club data for default prediction.\n",
        "\n",
        "    - Training/testing split is only on finalized loans.\n",
        "    - Current loans are kept separate for future scoring.\n",
        "    - Creates binary target 'default_flag' (1 = bad, 0 = good).\n",
        "    - Optional: oversample the minority class in the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cleaned_data: pd.DataFrame):\n",
        "        self.data = cleaned_data.copy()\n",
        "        self.finalized_df = None\n",
        "        self.current_df = None\n",
        "\n",
        "        # run preparation\n",
        "        self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Create default_flag and separate finalized and current loans.\"\"\"\n",
        "        bad_statuses = [\n",
        "            'Charged Off',\n",
        "            'Default',\n",
        "            'Does not meet the credit policy. Status:Charged Off',\n",
        "            'Late (31-120 days)',\n",
        "            'Late (16-30 days)',\n",
        "            'In Grace Period'\n",
        "        ]\n",
        "        good_statuses = [\n",
        "            'Fully Paid',\n",
        "            'Does not meet the credit policy. Status:Fully Paid'\n",
        "        ]\n",
        "\n",
        "        # Finalized loans: used for train/test split\n",
        "        self.finalized_df = self.data[self.data['loan_status'].isin(bad_statuses + good_statuses)].copy()\n",
        "        self.finalized_df['default_flag'] = np.where(self.finalized_df['loan_status'].isin(bad_statuses), 1, 0)\n",
        "\n",
        "        # Current loans: kept separately for scoring\n",
        "        self.current_df = self.data[self.data['loan_status'] == 'Current'].copy()\n",
        "\n",
        "        return self.finalized_df, self.current_df\n",
        "\n",
        "    def split_train_test(self, test_size: float = 0.2, random_state: int = 42, oversample: bool = False):\n",
        "        \"\"\"\n",
        "        Split finalized loans into training and testing sets.\n",
        "        Optionally oversample the minority class in the training set.\n",
        "        \"\"\"\n",
        "        if self.finalized_df is None:\n",
        "            raise ValueError(\"Call prepare_data() first.\")\n",
        "\n",
        "        X = self.finalized_df.drop(['loan_status', 'default_flag'], axis=1)\n",
        "        y = self.finalized_df['default_flag']\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "        )\n",
        "\n",
        "        return x_train, x_test, y_train, y_test\n",
        "\n",
        "    def get_finalized_and_current(self):\n",
        "        \"\"\"Return finalized (for training/testing) and current (for scoring) DataFrames.\"\"\"\n",
        "        if self.finalized_df is None or self.current_df is None:\n",
        "            raise ValueError(\"Call prepare_data() first.\")\n",
        "        return self.finalized_df, self.current_df\n",
        "\n"
      ],
      "metadata": {
        "id": "PcAQBASHyoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Feature Scaling"
      ],
      "metadata": {
        "id": "ADi2QOns0Sc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureScaler:\n",
        "    \"\"\"Feature Scaling class to apply various scaling techniques.\"\"\"\n",
        "\n",
        "    def __init__(self, x_train: pd.DataFrame, x_test: pd.DataFrame):\n",
        "        self.x_train = x_train\n",
        "        self.x_test = x_test\n",
        "\n",
        "    def _apply_scaler(self, scaler):\n",
        "        \"\"\"Internal method to apply any scaler to train and test data.\"\"\"\n",
        "        # Fit scaler on training data and transform\n",
        "        x_train_scaled = scaler.fit_transform(self.x_train)\n",
        "        # Transform test data using the same parameters\n",
        "        x_test_scaled = scaler.transform(self.x_test)\n",
        "\n",
        "        # Convert back to DataFrame (preserve column names/index)\n",
        "        x_train_scaled = pd.DataFrame(\n",
        "            x_train_scaled, columns=self.x_train.columns, index=self.x_train.index\n",
        "        )\n",
        "        x_test_scaled = pd.DataFrame(\n",
        "            x_test_scaled, columns=self.x_test.columns, index=self.x_test.index\n",
        "        )\n",
        "\n",
        "        return x_train_scaled, x_test_scaled, scaler\n",
        "\n",
        "\n",
        "    def standard_scaling(self, return_scaler=False):\n",
        "        \"\"\"Apply StandardScaler (Z-score normalization).\"\"\"\n",
        "        return self._apply_scaler(StandardScaler())\n",
        "\n",
        "    def min_max_scaling(self, feature_range=(0, 1), return_scaler=False):\n",
        "        \"\"\"Apply MinMaxScaler (scales features to a specified range).\"\"\"\n",
        "        return self._apply_scaler(MinMaxScaler(feature_range=feature_range))\n"
      ],
      "metadata": {
        "id": "wyeYpMbL0XUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selecting"
      ],
      "metadata": {
        "id": "sAu5Warp03dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureSelector:\n",
        "    \"\"\"Feature Selection class to reduce input features to the most relevant ones.\"\"\"\n",
        "\n",
        "    def __init__(self, x_train, x_test, y_train):\n",
        "        # Store training and testing feature sets and the target for supervised selection\n",
        "        self.x_train = x_train\n",
        "        self.x_test = x_test\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def select_k_best(self, score_func=f_classif, k=10):\n",
        "        \"\"\"Select top k features using SelectKBest (univariate feature selection).\"\"\"\n",
        "        # Initialize selector with given scoring function and number of features\n",
        "        selector = SelectKBest(score_func=score_func, k=k)\n",
        "\n",
        "        # Fit on training data and transform\n",
        "        x_train_selected = selector.fit_transform(self.x_train, self.y_train)\n",
        "\n",
        "        # Transform test data using same selected features\n",
        "        x_test_selected = selector.transform(self.x_test)\n",
        "        return x_train_selected, x_test_selected\n",
        "\n",
        "    def recursive_feature_elimination(self, estimator=None, n_features_to_select=10):\n",
        "        \"\"\"Use RFE to iteratively remove less important features (model-based).\"\"\"\n",
        "        if estimator is None:\n",
        "            # Default to Logistic Regression if no estimator is provided\n",
        "            estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        # Initialize RFE with estimator and number of features to select\n",
        "        selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select\n",
        "                       )\n",
        "        # Fit RFE on training data and transform\n",
        "        x_train_selected = selector.fit_transform(self.x_train, self.y_train)\n",
        "\n",
        "        # Transform test data using selected features\n",
        "        x_test_selected = selector.transform(self.x_test)\n",
        "        return x_train_selected, x_test_selected\n"
      ],
      "metadata": {
        "id": "C1ZBR5s-09av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Feature Engineering/Encoding"
      ],
      "metadata": {
        "id": "SQ_7sCHW1FhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer:\n",
        "    \"\"\"Feature Engineering class to create new features and encode categorical data.\"\"\"\n",
        "\n",
        "    def __init__(self, cleaned_data: pd.DataFrame):\n",
        "        # Store the dataset to be transformed\n",
        "        self.data = cleaned_data\n",
        "\n",
        "    def add_polynomial_features(self, degree=2, interaction_only=False, include_bias=False):\n",
        "        poly = PolynomialFeatures(degree=degree,\n",
        "                                interaction_only=interaction_only,\n",
        "                                include_bias=include_bias)\n",
        "        transformed = poly.fit_transform(self.data)\n",
        "        feature_names = poly.get_feature_names_out(self.data.columns)\n",
        "        df_transformed = pd.DataFrame(transformed, columns=feature_names, index=self.data.index)\n",
        "\n",
        "        return df_transformed, poly\n",
        "\n",
        "\n",
        "\n",
        "    def one_hot_encode(self, categorical_columns: list):\n",
        "        \"\"\"Fit OneHotEncoder on the dataset and return transformed DataFrame and transformer.\"\"\"\n",
        "        encoder = ColumnTransformer(\n",
        "            transformers=[('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_columns)],\n",
        "            remainder='passthrough'\n",
        "        )\n",
        "\n",
        "        # Fit and transform training data\n",
        "        transformed = encoder.fit_transform(self.data)\n",
        "\n",
        "        # If sparse matrix, convert to dense\n",
        "        if hasattr(transformed, \"toarray\"):\n",
        "            transformed = transformed.toarray()\n",
        "\n",
        "        # Get feature names and preserve index\n",
        "        feature_names = encoder.get_feature_names_out()\n",
        "        df_transformed = pd.DataFrame(transformed, columns=feature_names, index=self.data.index)\n",
        "\n",
        "        return df_transformed, encoder"
      ],
      "metadata": {
        "id": "wp-CjMpb1JIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Metric"
      ],
      "metadata": {
        "id": "2K0AMRKQ6PuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics:\n",
        "    \"\"\"Class for calculating and visualizing classification metrics with probability-based analysis and model diagnostics.\"\"\"\n",
        "\n",
        "    def __init__(self, x_train, x_test, y_train, y_test, model=None):\n",
        "        self.x_train = x_train\n",
        "        self.x_test = x_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.model = model  # CatBoost or any sklearn-like model\n",
        "\n",
        "    # ------------------------- Core Metric Plots -------------------------\n",
        "    def plot_confusion_matrix(self, y_true, y_pred, model_name=\"Model\"):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_roc_curve(self, y_true, y_proba, model_name=\"Model\"):\n",
        "        if y_proba is not None:\n",
        "            RocCurveDisplay.from_predictions(y_true, y_proba)\n",
        "            plt.title(f\"{model_name} - ROC Curve\")\n",
        "            plt.show()\n",
        "\n",
        "    # ------------------------- Metrics Calculation -------------------------\n",
        "    def get_classification_metrics(self, y_true, y_pred, y_proba=None, model_name=\"Model\", plot=True):\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred)\n",
        "        rec = recall_score(y_true, y_pred)\n",
        "        roc_auc = roc_auc_score(y_true, y_proba) if y_proba is not None else None\n",
        "        brier = brier_score_loss(y_true, y_proba) if y_proba is not None else None\n",
        "        mcc = matthews_corrcoef(y_true, y_pred)\n",
        "        kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "        results = {\n",
        "            \"Model\": model_name,\n",
        "            \"Accuracy\": acc,\n",
        "            \"F1\": f1,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"ROC-AUC\": roc_auc,\n",
        "            \"Cohen_Kappa\": kappa,\n",
        "            \"Brier_Score\": brier\n",
        "        }\n",
        "\n",
        "        if plot:\n",
        "            display(Markdown(f\"### {model_name} Metrics\"))\n",
        "            print(results)\n",
        "            self.plot_confusion_matrix(y_true, y_pred, model_name)\n",
        "            if y_proba is not None:\n",
        "                self.plot_roc_curve(y_true, y_proba, model_name)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    # ------------------------- Overfitting & Cross-Validation -------------------------\n",
        "    def cross_validation_check(self, cv=5, scoring='f1'):\n",
        "        if self.model is None:\n",
        "            print(\"No model provided for cross-validation.\")\n",
        "            return None\n",
        "        cv_scores = cross_val_score(self.model, self.x_train, self.y_train, cv=cv, scoring=scoring)\n",
        "        print(f\"Cross-Validation ({cv}-fold) {scoring.upper()} Scores: {cv_scores}\")\n",
        "        print(f\"Mean {scoring.upper()}: {np.mean(cv_scores):.4f}, Std: {np.std(cv_scores):.4f}\")\n",
        "        return cv_scores\n",
        "\n",
        "    def plot_learning_curve(self, cv=5, scoring='f1'):\n",
        "        if self.model is None:\n",
        "            print(\"No model provided for learning curve.\")\n",
        "            return\n",
        "        train_sizes, train_scores, val_scores = learning_curve(\n",
        "            self.model, self.x_train, self.y_train,\n",
        "            cv=cv, scoring=scoring, train_sizes=np.linspace(0.1, 1.0, 5)\n",
        "        )\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\"Train Score\")\n",
        "        plt.plot(train_sizes, np.mean(val_scores, axis=1), label=\"Validation Score\")\n",
        "        plt.xlabel(\"Training Size\")\n",
        "        plt.ylabel(scoring.upper())\n",
        "        plt.title(\"Learning Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # ------------------------- Calibration Check -------------------------\n",
        "    def plot_calibration_curve(self):\n",
        "        if self.model is None:\n",
        "            print(\"No model provided for calibration plot.\")\n",
        "            return\n",
        "        if hasattr(self.model, \"predict_proba\"):\n",
        "            y_prob = self.model.predict_proba(self.x_test)[:, 1]\n",
        "            prob_true, prob_pred = calibration_curve(self.y_test, y_prob, n_bins=10)\n",
        "            plt.figure(figsize=(5,4))\n",
        "            plt.plot(prob_pred, prob_true, marker='o', label='Calibration')\n",
        "            plt.plot([0,1],[0,1], linestyle='--', label='Perfectly Calibrated')\n",
        "            plt.xlabel(\"Predicted Probability\")\n",
        "            plt.ylabel(\"True Probability\")\n",
        "            plt.title(\"Calibration Plot\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Model does not support probability predictions.\")\n",
        "\n",
        "    # ------------------------- Data Leakage Check -------------------------\n",
        "    def check_data_leakage(self):\n",
        "        # Overlapping rows between train and test\n",
        "        overlap = pd.merge(self.x_train, self.x_test, how='inner')\n",
        "        print(f\"Overlapping rows between train and test: {len(overlap)}\")\n",
        "\n",
        "        # Feature correlation with target\n",
        "        corr_with_target = self.x_train.corrwith(self.y_train)\n",
        "        print(\"\\nTop 5 features correlated with target:\")\n",
        "        print(corr_with_target.sort_values(ascending=False).head())\n",
        "\n",
        "    # ------------------------- Feature Importance -------------------------\n",
        "    def plot_feature_importance(self):\n",
        "        if self.model is None:\n",
        "            print(\"No model provided for feature importance.\")\n",
        "            return\n",
        "        if hasattr(self.model, \"get_feature_importance\"):\n",
        "            importance = self.model.get_feature_importance()\n",
        "            features = self.x_train.columns\n",
        "            plt.figure(figsize=(6,8))\n",
        "            plt.barh(features, importance)\n",
        "            plt.xlabel(\"Feature Importance\")\n",
        "            plt.ylabel(\"Feature\")\n",
        "            plt.title(\"Feature Importance\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Model does not support feature importance.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3dvnWw0H54-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Models"
      ],
      "metadata": {
        "id": "rElAIpWh6LTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BuildClassifierModels:\n",
        "    \"\"\"Class to build, train, and evaluate classification models for loan default prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, x_train, x_test, y_train, y_test, feature_selector=None):\n",
        "        # Store train/test data\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Optional feature selector\n",
        "        self.feature_selector = feature_selector\n",
        "\n",
        "        # Metrics class for classification evaluation\n",
        "        self.metrics = Metrics(x_train, x_test, y_train, y_test)\n",
        "\n",
        "    def _train_and_evaluate(self, model, model_name=\"Model\", plot=True):\n",
        "        \"\"\"Train a classifier and evaluate using ROC-AUC, F1, Precision, Recall.\"\"\"\n",
        "        # Apply feature selection if provided\n",
        "        if self.feature_selector:\n",
        "            self.x_train = self.feature_selector.fit_transform(self.x_train, self.y_train)\n",
        "            self.x_test = self.feature_selector.transform(self.x_test)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(self.x_train, self.y_train)\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = model.predict(self.x_test)\n",
        "        y_proba = model.predict_proba(self.x_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        # Compute classification metrics\n",
        "        metrics = self.metrics.get_classification_metrics(\n",
        "            self.y_test, y_pred, y_proba=y_proba, model_name=model_name, plot=plot\n",
        "        )\n",
        "        return model, metrics\n",
        "\n",
        "    def random_forest(self, **kwargs):\n",
        "        return self._train_and_evaluate(\n",
        "            RandomForestClassifier(**kwargs), model_name=\"Random Forest\"\n",
        "        )\n",
        "\n",
        "    def xgboost(self, **kwargs):\n",
        "        return self._train_and_evaluate(\n",
        "            xgb.XGBClassifier(**kwargs), model_name=\"XGBoost\"\n",
        "        )\n",
        "\n",
        "    def lightgbm(self, **kwargs):\n",
        "        return self._train_and_evaluate(\n",
        "            lgb.LGBMClassifier(**kwargs), model_name=\"LightGBM\"\n",
        "        )\n",
        "\n",
        "    def catboost(self, **kwargs):\n",
        "        return self._train_and_evaluate(\n",
        "            CatBoostClassifier(verbose=0, **kwargs), model_name=\"CatBoost\"\n",
        "        )\n",
        "\n",
        "    # def stacking_classifier(self, estimators=None, final_estimator=None, **kwargs):\n",
        "    #     \"\"\"\n",
        "    #     Build, train, and evaluate a stacking classifier.\n",
        "    #     Default base estimators: Random Forest + XGBoost\n",
        "    #     Default meta-model: Logistic Regression\n",
        "    #     \"\"\"\n",
        "    #     from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "    #     from sklearn.linear_model import LogisticRegression\n",
        "    #     import xgboost as xgb\n",
        "\n",
        "    #     # Default base models\n",
        "    #     if estimators is None:\n",
        "    #         estimators = [\n",
        "    #             ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
        "    #             ('xgb', xgb.XGBClassifier(n_estimators=300, learning_rate=0.1, random_state=42))\n",
        "    #         ]\n",
        "\n",
        "    #     # Default meta-model\n",
        "    #     if final_estimator is None:\n",
        "    #         final_estimator = LogisticRegression()\n",
        "\n",
        "    #     stack_model = StackingClassifier(\n",
        "    #         estimators=estimators,\n",
        "    #         final_estimator=final_estimator,\n",
        "    #         **kwargs\n",
        "    #     )\n",
        "\n",
        "    #     return self._train_and_evaluate(stack_model, model_name=\"Stacking Classifier\")\n"
      ],
      "metadata": {
        "id": "C9DjeX9G2rR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing Pipeline"
      ],
      "metadata": {
        "id": "hIBv3Kp8660R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessorPipeline:\n",
        "    \"\"\"End-to-End Pipeline for Classification: Preprocessing, Feature Encoding, Feature Engineering, Feature Selection, and Model Training\"\"\"\n",
        "\n",
        "    def __init__(self, cleaned_data, save_metrics=False):\n",
        "        self.cleaned_data = cleaned_data\n",
        "        self.x_train = self.x_test = self.y_train = self.y_test = pd.DataFrame()\n",
        "\n",
        "        self.x_train_orig = self.x_test_orig = pd.DataFrame()\n",
        "        self.model_results = []\n",
        "        self.save = save_metrics\n",
        "        self.validation_df = pd.DataFrame()\n",
        "\n",
        "        # RUN\n",
        "        self.perform_data_spliting()\n",
        "\n",
        "    # ---------------- Data Splitting ----------------\n",
        "    def perform_data_spliting(self, test_size=0.2, random_state=42) -> tuple:\n",
        "        \"\"\"Split into train and test sets.\"\"\"\n",
        "        splitter = DataSplitter(cleaned_data=self.cleaned_data)\n",
        "        self.validation_df = splitter.current_df\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = splitter.split_train_test()\n",
        "\n",
        "        self.x_train_orig = self.x_train.copy()\n",
        "        self.x_test_orig = self.x_test.copy()\n",
        "        return self.x_train, self.x_test, self.y_train, self.y_test\n",
        "\n",
        "    # ---------------- Feature Encoding ----------------\n",
        "    def perform_feature_encoding(self, categorical_columns: list, save_transformer=True, transformer_filename=\"encoder.joblib\"):\n",
        "        \"\"\"Encode categorical features using one-hot encoding and optionally save transformer.\"\"\"\n",
        "        if categorical_columns:\n",
        "            print(f\"\\n🧩 Encoding categorical columns: {categorical_columns}\")\n",
        "            encoder = FeatureEngineer(self.x_train)\n",
        "            self.x_train, transformer = encoder.one_hot_encode(categorical_columns)\n",
        "\n",
        "            transformed_test = transformer.transform(self.x_test)\n",
        "            if hasattr(transformed_test, \"toarray\"):\n",
        "                transformed_test = transformed_test.toarray()\n",
        "\n",
        "            feature_names = transformer.get_feature_names_out()\n",
        "            self.x_test = pd.DataFrame(transformed_test, columns=feature_names, index=self.x_test.index)\n",
        "\n",
        "            # 🔹 Save transformer for later use\n",
        "            if save_transformer:\n",
        "                joblib.dump(transformer, transformer_filename)\n",
        "                print(f\"✅ Encoder transformer saved as {transformer_filename}\")\n",
        "\n",
        "\n",
        "    # ---------------- Feature Scaling ----------------\n",
        "    def perform_features_scaling(self, method='standard', save_scaler=True, scaler_filename=\"scaler.joblib\"):\n",
        "        \"\"\"Scale numeric features and optionally save scaler.\"\"\"\n",
        "        print(f\"\\n⚖️ Scaling features using: {method}\")\n",
        "        scaler = FeatureScaler(self.x_train, self.x_test)\n",
        "        if method == 'standard':\n",
        "            self.x_train, self.x_test, scaler_obj = scaler.standard_scaling(return_scaler=True)\n",
        "        elif method == 'minmax':\n",
        "            self.x_train, self.x_test, scaler_obj = scaler.min_max_scaling(return_scaler=True)\n",
        "\n",
        "        # 🔹 Save scaler for later use\n",
        "        if save_scaler:\n",
        "            joblib.dump(scaler_obj, scaler_filename)\n",
        "            print(f\"✅ Scaler saved as {scaler_filename}\")\n",
        "\n",
        "\n",
        "    # ---------------- Feature Selection ----------------\n",
        "    def perform_feature_selection(self, method='k_best', k=10):\n",
        "        \"\"\"Select top k features to reduce dimensionality.\"\"\"\n",
        "        print(f\"\\n📌 Selecting top {k} features using {method}...\")\n",
        "        selector = FeatureSelector(self.x_train, self.x_test, self.y_train)\n",
        "        if method == 'k_best':\n",
        "            self.x_train, self.x_test = selector.select_k_best(k=k)\n",
        "        elif method == 'rfe':\n",
        "            self.x_train, self.x_test = selector.recursive_feature_elimination(n_features_to_select=k)\n",
        "\n",
        "    # ---------------- Feature Engineering ----------------\n",
        "    def perform_feature_engineering(self, use_polynomial=False, poly_degree=2):\n",
        "        \"\"\"Optionally add polynomial features.\"\"\"\n",
        "        if use_polynomial:\n",
        "            print(f\"\\n🔧 Applying polynomial feature engineering (degree={poly_degree})\")\n",
        "            fe_train = FeatureEngineer(pd.DataFrame(self.x_train))\n",
        "            self.x_train = fe_train.add_polynomial_features(degree=poly_degree)\n",
        "\n",
        "            fe_test = FeatureEngineer(pd.DataFrame(self.x_test))\n",
        "            self.x_test = fe_test.add_polynomial_features(degree=poly_degree)\n",
        "\n",
        "    # ---------------- Model Training ----------------\n",
        "    def create_models(self, save_model=False):\n",
        "        \"\"\"Train and evaluate multiple classification models.\"\"\"\n",
        "        print(\"\\n🤖 Training classification models...\")\n",
        "        builder = BuildClassifierModels(self.x_train, self.x_test, self.y_train, self.y_test)\n",
        "\n",
        "        for name, func in {\n",
        "            # 'RandomForest': builder.random_forest,\n",
        "            # 'XGBoost': builder.xgboost,\n",
        "            # 'LightGBM': builder.lightgbm,\n",
        "            'CatBoost': builder.catboost,\n",
        "            # 'Stacking': builder.stacking_classifier\n",
        "        }.items():\n",
        "            model, metrics = func()\n",
        "            self.model_results.append({\"Model\": name, **metrics})\n",
        "\n",
        "            # Save the trained model\n",
        "            if save_model:\n",
        "                self.save_model(model, model_name=name)\n",
        "\n",
        "        # After training → save + display results\n",
        "        self.save_and_display_results()\n",
        "\n",
        "    # ---------------- Save & Display ----------------\n",
        "    def save_and_display_results(self, filename=\"classification_results.csv\"):\n",
        "        \"\"\"Save results to CSV and print Markdown table.\"\"\"\n",
        "        df = pd.DataFrame(self.model_results)\n",
        "        df = df.round(3)\n",
        "\n",
        "        if self.save:\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"\\n📂 Results saved to {filename}\")\n",
        "\n",
        "        print(\"\\n### 📊 Classification Model Performance\\n\")\n",
        "        print(df.to_markdown(index=False))\n",
        "\n",
        "    # ---------------- Save Trained Model ----------------\n",
        "    def save_model(self, model, model_name=\"model\", filename=None):\n",
        "        \"\"\"Save a trained model to disk.\"\"\"\n",
        "        if filename is None:\n",
        "            filename = f\"{model_name}.joblib\"\n",
        "        joblib.dump(model, filename)\n",
        "        print(f\"✅ Model '{model_name}' saved as {filename}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "X-dAihkW62K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an instance of the pipeline"
      ],
      "metadata": {
        "id": "SAJXOnCc_Xk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create pipeline instance\n",
        "pipeline = PreprocessorPipeline(cleaned_data=cleaned_accepted_df)"
      ],
      "metadata": {
        "id": "fOprd5xm9qr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🕸️ Train Models with some numerical variables as input features (most important)"
      ],
      "metadata": {
        "id": "pn29dpadAc2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Using some important numeric columns\n",
        "# numeric_features = [\n",
        "#     \"loan_amnt\",\n",
        "#     \"funded_amnt\",\n",
        "#     \"funded_amnt_inv\",\n",
        "#     \"annual_inc\",\n",
        "#     \"dti\",\n",
        "#     \"fico_range_low\",\n",
        "#     \"fico_range_high\",\n",
        "#     \"open_acc\",\n",
        "#     \"revol_bal\",\n",
        "#     \"total_acc\",\n",
        "#     \"acc_open_past_24mths\",\n",
        "#     \"avg_cur_bal\",\n",
        "#     \"bc_open_to_buy\",\n",
        "#     \"bc_util\",\n",
        "#     \"delinq_2yrs\",\n",
        "#     \"inq_last_6mths\",\n",
        "#     \"installment\",\n",
        "#     \"mo_sin_old_il_acct\",\n",
        "#     \"tot_cur_bal\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# pipeline.x_train = pipeline.x_train_orig[numeric_features]\n",
        "# pipeline.x_test = pipeline.x_test_orig[numeric_features]"
      ],
      "metadata": {
        "id": "SnJXxpz8Anw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform feature scaling\n",
        "# pipeline.perform_features_scaling(method='standard')"
      ],
      "metadata": {
        "id": "O-JlD2ITyJG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clear model results for next models\n",
        "# pipeline.model_results = []\n",
        "\n",
        "# # train models\n",
        "# pipeline.create_models()"
      ],
      "metadata": {
        "id": "tXWH5-3aB8rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🕸️ Train models that uses all numeric features"
      ],
      "metadata": {
        "id": "9duwIrcuCe5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # build with all numeric feautures\n",
        "# pipeline.x_train = pipeline.x_train_orig.select_dtypes(include='number')\n",
        "# pipeline.x_test = pipeline.x_test_orig.select_dtypes(include='number')\n"
      ],
      "metadata": {
        "id": "VQQYsNLDsVbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # perform feature scaling\n",
        "# pipeline.perform_features_scaling(method='standard')"
      ],
      "metadata": {
        "id": "TN7AlGcfyUB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Clear model results for next models\n",
        "# pipeline.model_results = []\n",
        "\n",
        "# # # train models\n",
        "# pipeline.create_models()"
      ],
      "metadata": {
        "id": "PDHxnoQ3yoyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🕸️ Train models that uses all relevant input variables (both categorical and numerical)"
      ],
      "metadata": {
        "id": "PHB9mOIFCUdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform features encoding\n",
        "pipeline.x_train = pipeline.x_train_orig\n",
        "pipeline.x_test = pipeline.x_test_orig\n",
        "categorical_columns = list(pipeline.x_train.select_dtypes(exclude='number').columns)\n",
        "print(pipeline.x_train.shape)\n",
        "pipeline.perform_feature_encoding(categorical_columns=categorical_columns)\n",
        "print(pipeline.x_train.shape)"
      ],
      "metadata": {
        "id": "Tnj8dQg93FsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform feature scaling\n",
        "pipeline.perform_features_scaling(method='standard')"
      ],
      "metadata": {
        "id": "JctOird64BUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clear model results for next models\n",
        "pipeline.model_results = []\n",
        "\n",
        "# # train models\n",
        "# pipeline.create_models(save_model=True)"
      ],
      "metadata": {
        "id": "H_mLjVrsNjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧑‍💻 Perform HPO on BEST MODEL\n",
        "\n",
        "> For now the model is fine, no need for HPO"
      ],
      "metadata": {
        "id": "citE12P2cEuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❄️ Develop an Artificial Neural Network (ANN)\n",
        "If the performance after HPO is still not good, build an ANN\n",
        "\n",
        "> For now the CatBoost is perfoming really good"
      ],
      "metadata": {
        "id": "cVOSr1ilDFTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅🕸️ Use the model to Predict new data"
      ],
      "metadata": {
        "id": "UporOdzVH4wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor:\n",
        "    \"\"\"Predictor class that preprocesses and predicts using a trained model, with schema validation.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, model_filename, df_schema,\n",
        "                 use_scale=True, use_encode=True,\n",
        "                 use_polynomial=False, poly_degree=2):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - df: initial dataframe to predict on\n",
        "        - model_filename: str, path to saved model (.joblib)\n",
        "        - df_schema: pandas.DataFrame, dataframe with expected columns and types\n",
        "        - use_scale: bool, whether to scale numeric features\n",
        "        - use_encode: bool, whether to one-hot encode categorical features\n",
        "        - use_polynomial: bool, whether to apply polynomial features\n",
        "        - poly_degree: int, degree of polynomial features\n",
        "        \"\"\"\n",
        "        self.model_filename = model_filename\n",
        "        self.model = joblib.load(model_filename)\n",
        "\n",
        "        self.use_scale = use_scale\n",
        "        self.use_encode = use_encode\n",
        "        self.use_polynomial = use_polynomial\n",
        "        self.poly_degree = poly_degree\n",
        "\n",
        "        self.schema_cols = df_schema.columns.tolist()\n",
        "        self.schema_dtypes = df_schema.dtypes.to_dict()\n",
        "        self.categorical_columns = list(df_schema.select_dtypes(exclude='number').columns)\n",
        "\n",
        "        self.data = df  # current data to predict on\n",
        "\n",
        "    # ---------------- Schema Validation ----------------\n",
        "    def validate_schema(self):\n",
        "        \"\"\"Check that the dataframe has the expected columns and types.\"\"\"\n",
        "        missing_cols = [c for c in self.schema_cols if c not in self.data.columns]\n",
        "        extra_cols = [c for c in self.data.columns if c not in self.schema_cols]\n",
        "\n",
        "        if missing_cols or extra_cols:\n",
        "            raise ValueError(\n",
        "                f\"Schema mismatch detected!\\nMissing columns: {missing_cols}\\nExtra columns: {extra_cols}\"\n",
        "            )\n",
        "\n",
        "        for col, dtype in self.schema_dtypes.items():\n",
        "            if not pd.api.types.is_dtype_equal(self.data[col].dtype, dtype):\n",
        "                raise TypeError(f\"Column '{col}' has wrong type. Expected {dtype}, got {self.data[col].dtype}\")\n",
        "\n",
        "    # ---------------- Feature Encoding ----------------\n",
        "    def perform_feature_encoding(self):\n",
        "        \"\"\"Encode categorical features using one-hot encoding.\"\"\"\n",
        "        if self.categorical_columns:\n",
        "            encoder = FeatureEngineer(self.data)\n",
        "            X_encoded, transformer = encoder.one_hot_encode(self.categorical_columns)\n",
        "            self.data = X_encoded\n",
        "\n",
        "    # ---------------- Feature Scaling ----------------\n",
        "    def perform_features_scaling(self, method='standard'):\n",
        "        \"\"\"Scale numeric features.\"\"\"\n",
        "        scaler = FeatureScaler(self.data, self.data)\n",
        "        if method == 'standard':\n",
        "            self.data, _ = scaler.standard_scaling()\n",
        "\n",
        "    # ---------------- Preprocess ----------------\n",
        "    def preprocess(self):\n",
        "        \"\"\"Apply preprocessing: encoding, scaling, polynomial features.\"\"\"\n",
        "        self.validate_schema()\n",
        "\n",
        "        if self.use_encode:\n",
        "            self.perform_feature_encoding()\n",
        "\n",
        "        if self.use_scale:\n",
        "            self.perform_features_scaling()\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    # ---------------- Predict on Existing DataFrame ----------------\n",
        "    def predict_existing_df(self, add_proba=True,\n",
        "                            col_pred=\"Prediction\", col_proba=\"Prediction_Proba\"):\n",
        "        \"\"\"\n",
        "        Predict for the dataframe passed at init (self.data).\n",
        "        Automatically validates schema and preprocesses.\n",
        "        \"\"\"\n",
        "        X_proc = self.preprocess()\n",
        "\n",
        "        df = self.data.copy()  # so we don’t overwrite original directly\n",
        "\n",
        "        print(df.shape)\n",
        "\n",
        "        df[col_pred] = self.model.predict(X_proc)\n",
        "        if add_proba and hasattr(self.model, \"predict_proba\"):\n",
        "            df[col_proba] = self.model.predict_proba(X_proc)[:, 1]\n",
        "\n",
        "        return df\n",
        "\n",
        "    # ---------------- Predict on New Dataset ----------------\n",
        "    def predict_new_data(self, X_new, add_proba=True,\n",
        "                         col_pred=\"Prediction\", col_proba=\"Prediction_Proba\"):\n",
        "        \"\"\"\n",
        "        Predict on a completely new dataset.\n",
        "        Automatically validates schema and preprocesses.\n",
        "        \"\"\"\n",
        "        self.data = X_new\n",
        "        X_proc = self.preprocess()\n",
        "\n",
        "        df = self.data.copy()\n",
        "        df[col_pred] = self.model.predict(X_proc)\n",
        "        if add_proba and hasattr(self.model, \"predict_proba\"):\n",
        "            df[col_proba] = self.model.predict_proba(X_proc)[:, 1]\n",
        "\n",
        "        return df\n",
        "\n"
      ],
      "metadata": {
        "id": "eqC0uufLH0zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Predictor"
      ],
      "metadata": {
        "id": "wXRj4mhFUvEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict validation data\n",
        "validation_df = pipeline.validation_df.head()\n"
      ],
      "metadata": {
        "id": "MBpxrHFrUVht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict for the dataframe we kept for validation"
      ],
      "metadata": {
        "id": "swhoP0rrUzRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the loan_status col\n",
        "validation_df = pipeline.validation_df.copy()\n",
        "validation_df = validation_df.drop(['loan_status'], axis=1)\n",
        "print(validation_df.shape, pipeline.x_train_orig.shape)\n",
        "\n",
        "categorical_columns = list(validation_df.select_dtypes(exclude='number').columns)\n",
        "print(len(categorical_columns))\n",
        "\n",
        "if categorical_columns:\n",
        "        print(f\"\\n🧩 Encoding categorical columns\")\n",
        "        encoder = FeatureEngineer(pipeline.x_train_orig)\n",
        "        validation_df, transformer = encoder.one_hot_encode(categorical_columns)\n",
        "\n",
        "validation_df.shape"
      ],
      "metadata": {
        "id": "_OWU_aJxen3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict for a new DataFrame"
      ],
      "metadata": {
        "id": "nYFEX7HEVSTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will raise an error if X_new does not match schema\n",
        "# y_pred, y_proba = predictor.predict_new_data(X_new)"
      ],
      "metadata": {
        "id": "0YGytXY_VCbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}